{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b9ae66b",
   "metadata": {},
   "source": [
    "<font color = teal>**Agentic System for Literature-Grounded Causal Reasoning**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fe3287",
   "metadata": {},
   "source": [
    "**Workflow Overview**\n",
    "\n",
    "```\n",
    "User Question\n",
    "      ↓\n",
    "LLM builds search query\n",
    "(causal-focused keywords)\n",
    "      ↓\n",
    "Search scientific literature (arXiv)\n",
    "      ↓\n",
    "Extract causal sentences\n",
    "(LLM, strict JSON output)\n",
    "      ↓\n",
    "Build causal graph\n",
    "(from extracted evidence)\n",
    "      ↓\n",
    "Evidence-based reasoning\n",
    "(over the causal graph)\n",
    "      ↓\n",
    "Verdict + explanation + uncertainty\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36a4d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silence noisy sklearn/arXiv warnings and improve Jupyter output scrolling\n",
    "\n",
    "import warnings\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<style>\n",
    ".output_scroll { max-height: 600px !important; }\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65780ad7",
   "metadata": {},
   "source": [
    "Imports: LLM client, literature search, and text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67258208",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import arxiv\n",
    "import re\n",
    "import networkx as nx\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4efde7a",
   "metadata": {},
   "source": [
    "1. Initialize OpenAI client using an API key.\n",
    "-  The API key is generated from https://platform.openai.com/account/api-keys\n",
    "and should be stored securely (e.g., as an environment variable), not hard-coded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fa58e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350ccf5a",
   "metadata": {},
   "source": [
    "2. LLM-Assisted Literature Search Query Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453f63da",
   "metadata": {},
   "source": [
    "- This function uses a large language model to convert a causal question about two variables into a concise, causality-focused arXiv search query.\n",
    "- The generated query is optimized to retrieve scientific papers that discuss whether intervening on one variable affects the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2896184c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_build_search_query(client, X: str, Y: str) -> str:\n",
    "    prompt = f\"\"\"\n",
    "You are constructing a search query for scientific literature.\n",
    "\n",
    "Goal:\n",
    "Find papers discussing whether changing or intervening on X affects Y.\n",
    "\n",
    "Variables:\n",
    "X = {X}\n",
    "Y = {Y}\n",
    "\n",
    "Include keywords such as:\n",
    "causal, effect, influence, intervention, mechanism, impact, change, risk, dependency\n",
    "\n",
    "Return ONLY a concise arXiv-style query string.\n",
    "\"\"\"\n",
    "    resp = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0,\n",
    "        max_tokens=40\n",
    "    )\n",
    "    return resp.choices[0].message.content.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18500095",
   "metadata": {},
   "source": [
    "2. Literature Retrieval from arXiv\n",
    "\n",
    "- This function searches the arXiv database using a provided query and returns the most relevant scientific papers up to a specified limit.\n",
    "- For each paper, it extracts key metadata—including the title, abstract, arXiv ID, DOI (if available), and publication date—to support downstream evidence extraction and causal reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb57763",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_arxiv(query: str, max_results: int = 15):\n",
    "    client = arxiv.Client()\n",
    "    search = arxiv.Search(\n",
    "        query=query,\n",
    "        max_results=max_results,\n",
    "        sort_by=arxiv.SortCriterion.Relevance\n",
    "    )\n",
    "\n",
    "    papers = []\n",
    "    for r in client.results(search):\n",
    "        papers.append({\n",
    "            \"title\": r.title,\n",
    "            \"abstract\": r.summary,\n",
    "            \"url\": r.entry_id,\n",
    "            \"arxiv_id\": r.entry_id.split(\"/\")[-1],\n",
    "            \"doi\": getattr(r, \"doi\", None),\n",
    "            \"published\": getattr(r, \"published\", None),\n",
    "            \"source\": \"arXiv\"\n",
    "        })\n",
    "    return papers\n",
    "\n",
    "\n",
    "# # Split scientific text into clean, individual sentences for evidence extraction\n",
    "\n",
    "_SENT_SPLIT = re.compile(r'(?<=[.!?])\\s+')\n",
    "\n",
    "def split_sentences(text: str):\n",
    "    text = re.sub(r\"\\s+\", \" \", (text or \"\").strip())\n",
    "    return [s.strip() for s in _SENT_SPLIT.split(text) if s.strip()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dd95d3",
   "metadata": {},
   "source": [
    "3. Causal Evidence Extraction from Scientific Abstracts\n",
    "- This function uses a language model to scan paper abstracts and extract exact sentences that support a causal relationship from X to Y, based on an intervention-style definition of causality.\n",
    "- The model is constrained to return strict JSON, enabling reliable parsing and ensuring that only explicit, sentence-level evidence is collected for downstream reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b2924d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_causal_evidence(client, X: str, Y: str, papers: list, max_sentences_per_paper: int = 3):\n",
    "    evidence = []\n",
    "\n",
    "    for p in tqdm(papers, desc=\"Extracting causal evidence\", unit=\"paper\"):\n",
    "        sentences = split_sentences(p[\"abstract\"])\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "You are extracting STRICT causal evidence from a scientific abstract.\n",
    "\n",
    "We are ONLY interested in sentences that EXPLICITLY assert\n",
    "a causal effect from X to Y.\n",
    "\n",
    "STRICT DEFINITION:\n",
    "A sentence supports \"X → Y\" ONLY IF it states that:\n",
    "- changing, modifying, intervening on, or manipulating X\n",
    "- directly causes, leads to, increases, decreases, or determines Y\n",
    "\n",
    "STRICT MATCHING RULE (MANDATORY):\n",
    "\n",
    "- The sentence MUST explicitly mention X or a clear synonym of X\n",
    "- AND MUST explicitly mention Y or a clear synonym of Y\n",
    "- If Y is not explicitly mentioned, DO NOT extract the sentence\n",
    "- Do NOT infer Y from downstream effects, intermediate variables, or context\n",
    "\n",
    "\n",
    "DO NOT extract sentences that:\n",
    "- merely mention association, correlation, relevance, or importance\n",
    "- describe examples, applications, goals, preferences, or optimization\n",
    "- discuss usage, scheduling, or motivation without causal testing\n",
    "- imply causality without explicit causal verbs\n",
    "- reverse the direction (Y → X)\n",
    "\n",
    "X = \"{X}\"\n",
    "Y = \"{Y}\"\n",
    "\n",
    "ALLOWED causal verbs (must appear clearly):\n",
    "\"causes\", \"leads to\", \"results in\", \"induces\", \"increases\", \"decreases\",\n",
    "\"affects\", \"determines\", \"controls\", \"drives\", \"modulates\"\n",
    "\n",
    "TASK:\n",
    "From the sentences below, extract up to {max_sentences_per_paper}\n",
    "sentences that STRICTLY satisfy the definition above.\n",
    "\n",
    "OUTPUT FORMAT (STRICT JSON ONLY):\n",
    "\n",
    "If supporting sentences exist:\n",
    "{{\n",
    "  \"support_sentences\": [\"exact sentence 1\", \"exact sentence 2\"],\n",
    "  \"note\": \"short justification (optional)\"\n",
    "}}\n",
    "\n",
    "If NO sentence strictly satisfies the definition:\n",
    "{{\n",
    "  \"support_sentences\": [],\n",
    "  \"note\": \"no explicit causal claim\"\n",
    "}}\n",
    "\n",
    "SENTENCES:\n",
    "{json.dumps(sentences, ensure_ascii=False)}\n",
    "\"\"\"\n",
    "\n",
    "        resp = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0,\n",
    "            max_tokens=250\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            obj = json.loads(resp.choices[0].message.content)\n",
    "            for s in obj.get(\"support_sentences\", []):\n",
    "                evidence.append({\n",
    "                    \"sentence\": s,\n",
    "                    \"title\": p[\"title\"],\n",
    "                    \"arxiv_id\": p[\"arxiv_id\"],\n",
    "                    \"doi\": p[\"doi\"],\n",
    "                    \"url\": p[\"url\"]\n",
    "                })\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return evidence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a736d16f",
   "metadata": {},
   "source": [
    "4. Evidence-Based Causal Graph Construction\n",
    "\n",
    "- This builds a directed causal graph where edges are supported by extracted sentences from the literature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e347b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LiteratureCausalGraph:\n",
    "    def __init__(self):\n",
    "        self.graph = nx.DiGraph()\n",
    "        self.edge_evidence = {}\n",
    "\n",
    "    def add_edge(self, src, tgt, ev):\n",
    "        self.graph.add_edge(src, tgt)\n",
    "        self.edge_evidence.setdefault((src, tgt), []).append(ev)\n",
    "\n",
    "\n",
    "def build_graph_from_evidence(X: str, Y: str, evidence: list):\n",
    "    g = LiteratureCausalGraph()\n",
    "    for ev in evidence:\n",
    "        g.add_edge(X, Y, ev)\n",
    "    return g\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34af8288",
   "metadata": {},
   "source": [
    "5. Evidence-Based Causal Reasoning and Verdict\n",
    "\n",
    "- This function evaluates the extracted literature evidence to determine whether there is explicit support for a causal relationship from X to Y.\n",
    "- It produces a clear verdict along with cited evidence, a summary of the papers searched, and an explicit statement of uncertainty and limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857fc0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def literature_reasoning(graph: LiteratureCausalGraph, X: str, Y: str, papers: list):\n",
    "    n = len(papers)\n",
    "    edge = (X, Y)\n",
    "    found = graph.edge_evidence.get(edge, [])\n",
    "\n",
    "    paper_refs = []\n",
    "    for i, p in enumerate(papers, 1):\n",
    "        ref = f\"arXiv:{p['arxiv_id']}\"\n",
    "        if p.get(\"doi\"):\n",
    "            ref += f\" | DOI:{p['doi']}\"\n",
    "        paper_refs.append(f\"[{i}] {p['title']} ({ref})\")\n",
    "\n",
    "    if found:\n",
    "        ev_lines = []\n",
    "        for i, ev in enumerate(found, 1):\n",
    "            ref = f\"arXiv:{ev['arxiv_id']}\"\n",
    "            if ev.get(\"doi\"):\n",
    "                ref += f\", DOI:{ev['doi']}\"\n",
    "            ev_lines.append(f'({i}) \"{ev[\"sentence\"]}\" — {ref}')\n",
    "\n",
    "        return {\n",
    "            \"verdict\": \"YES (literature-supported)\",\n",
    "            \"reason\": (\n",
    "                f\"Searched {n} arXiv papers.\\n\\n\"\n",
    "                \"Papers searched:\\n\" + \"\\n\".join(paper_refs) + \"\\n\\n\"\n",
    "                f\"Supporting sentences for {X} → {Y}:\\n\" + \"\\n\".join(ev_lines)\n",
    "            ),\n",
    "            \"uncertainty\": (\n",
    "                \"Evidence is extracted from abstracts only.\\n\"\n",
    "                \"Some abstracts describe associations rather than interventions.\\n\"\n",
    "                \"Stronger claims require full-text analysis and study design validation.\"\n",
    "            )\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        \"verdict\": \"NO DIRECT ABSTRACT EVIDENCE\",\n",
    "        \"reason\": (\n",
    "            f\"Searched {n} arXiv papers.\\n\\n\"\n",
    "            \"Papers searched:\\n\" + \"\\n\".join(paper_refs) + \"\\n\\n\"\n",
    "            f\"No abstract sentence explicitly supported {X} → {Y}.\"\n",
    "        ),\n",
    "        \"uncertainty\": (\n",
    "            \"Absence of evidence ≠ absence of causality.\\n\"\n",
    "            \"The claim may exist in full text or non-arXiv sources (e.g., journals, patents).\"\n",
    "        )\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ea2511",
   "metadata": {},
   "source": [
    "6. Visualization of the Literature-Derived Causal Graph\n",
    "\n",
    "- This function visualizes the causal graph constructed from literature evidence, displaying variables as nodes and supported causal relationships as directed edges.\n",
    "- Edges are drawn with clear arrowheads and spacing to ensure interpretability, making the evidence-backed causal structure easy to inspect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dff2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_literature_graph(lit_graph: LiteratureCausalGraph):\n",
    "#     G = lit_graph.graph\n",
    "#     if G.number_of_edges() == 0:\n",
    "#         print(\"Graph is empty.\")\n",
    "#         return\n",
    "\n",
    "#     pos = nx.spring_layout(G, seed=42, k=0.6)\n",
    "\n",
    "#     plt.figure(figsize=(4.5, 4.5))\n",
    "\n",
    "#     nx.draw_networkx_nodes(\n",
    "#         G, pos,\n",
    "#         node_size=3000,\n",
    "#         node_color=\"lightblue\",\n",
    "#         edgecolors=\"black\"\n",
    "#     )\n",
    "\n",
    "#     nx.draw_networkx_edges(\n",
    "#         G, pos,\n",
    "#         arrows=True,\n",
    "#         arrowstyle=\"-|>\",\n",
    "#         arrowsize=18,\n",
    "#         width=2,\n",
    "#         edge_color=\"red\",\n",
    "#         min_source_margin=40,\n",
    "#         min_target_margin=40\n",
    "#     )\n",
    "\n",
    "#     nx.draw_networkx_labels(G, pos, font_size=10, font_weight=\"bold\")\n",
    "#     plt.margins(0.50)\n",
    "#     plt.title(\"Literature-Derived Causal Graph\")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "def plot_literature_graph(\n",
    "    lit_graph: LiteratureCausalGraph,\n",
    "    save_path: str = None,\n",
    "    show_empty: bool = True,\n",
    "):\n",
    "    G = lit_graph.graph\n",
    "\n",
    "    # ---- Handle empty graph case safely ----\n",
    "    if G.number_of_edges() == 0:\n",
    "        print(\"Graph is empty.\")\n",
    "\n",
    "        if show_empty:\n",
    "            fig, ax = plt.subplots(figsize=(4.5, 4.5))\n",
    "            ax.text(\n",
    "                0.5, 0.5,\n",
    "                \"No causal edges\\nfound in literature\",\n",
    "                ha=\"center\",\n",
    "                va=\"center\",\n",
    "                fontsize=12,\n",
    "                fontweight=\"bold\",\n",
    "                transform=ax.transAxes\n",
    "            )\n",
    "            ax.set_title(\"Literature-Derived Causal Graph\")\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "            if save_path is not None:\n",
    "                fig.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "            return fig\n",
    "\n",
    "        return None\n",
    "\n",
    "    # ---- Non-empty graph plotting ----\n",
    "    pos = nx.spring_layout(G, seed=42, k=0.6)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(4.5, 4.5))\n",
    "\n",
    "    nx.draw_networkx_nodes(\n",
    "        G, pos,\n",
    "        ax=ax,\n",
    "        node_size=3000,\n",
    "        node_color=\"lightblue\",\n",
    "        edgecolors=\"black\"\n",
    "    )\n",
    "\n",
    "    nx.draw_networkx_edges(\n",
    "        G, pos,\n",
    "        ax=ax,\n",
    "        arrows=True,\n",
    "        arrowstyle=\"-|>\",\n",
    "        arrowsize=18,\n",
    "        width=2,\n",
    "        edge_color=\"red\",\n",
    "        min_source_margin=40,\n",
    "        min_target_margin=40\n",
    "    )\n",
    "\n",
    "    nx.draw_networkx_labels(\n",
    "        G, pos,\n",
    "        ax=ax,\n",
    "        font_size=10,\n",
    "        font_weight=\"bold\"\n",
    "    )\n",
    "\n",
    "    ax.margins(0.50)\n",
    "    ax.set_title(\"Literature-Derived Causal Graph\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    if save_path is not None:\n",
    "        fig.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd7c04b",
   "metadata": {},
   "source": [
    "7. Running the Literature-Grounded Causal Analysis\n",
    "\n",
    "- This block executes the full causal reasoning workflow end to end: it generates a literature search query for the variables of interest, retrieves relevant papers, extracts causal evidence, constructs a causal graph, and produces a final verdict with supporting citations and uncertainty.\n",
    "- By changing the values of X and Y, the same pipeline can be reused to analyze different causal hypotheses in a consistent and transparent manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7d36ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = \"cation substitution\"\n",
    "# Y = \"lattice parameter\"\n",
    "\n",
    "# query = gpt_build_search_query(client, X, Y)\n",
    "# papers = search_arxiv(query, max_results=50)\n",
    "# evidence = extract_causal_evidence(client, X, Y, papers)\n",
    "# lit_graph = build_graph_from_evidence(X, Y, evidence)\n",
    "\n",
    "# result = literature_reasoning(lit_graph, X, Y, papers)\n",
    "\n",
    "# print(\"\\nVERDICT:\", result[\"verdict\"])\n",
    "# print(\"\\nREASONING:\\n\", result[\"reason\"])\n",
    "# print(\"\\nUNCERTAINTY:\\n\", result[\"uncertainty\"])\n",
    "\n",
    "# plot_literature_graph(lit_graph)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2326053",
   "metadata": {},
   "source": [
    "saving and research table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df96fb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_name(text):\n",
    "    return text.lower().replace(\" \", \"_\").replace(\"/\", \"_\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845a1401",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_and_save_literature_reasoning(\n",
    "    client,\n",
    "    X,\n",
    "    Y,\n",
    "    base_dir=\"llm_outputs\",\n",
    "    max_results=20\n",
    "):\n",
    "    # ---- folder setup ----\n",
    "    query_name = f\"{safe_name(X)}__{safe_name(Y)}\"\n",
    "    output_dir = os.path.join(base_dir, \"queries\", query_name)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # ---- pipeline ----\n",
    "    query = gpt_build_search_query(client, X, Y)\n",
    "    papers = search_arxiv(query, max_results=max_results)\n",
    "    evidence = extract_causal_evidence(client, X, Y, papers)\n",
    "    lit_graph = build_graph_from_evidence(X, Y, evidence)\n",
    "    result = literature_reasoning(lit_graph, X, Y, papers)\n",
    "\n",
    "    # ---- save JSON outputs ----\n",
    "    verdict_path = os.path.join(output_dir, \"verdict.json\")\n",
    "    with open(verdict_path, \"w\") as f:\n",
    "        json.dump(result, f, indent=2)\n",
    "\n",
    "    metadata = {\n",
    "        \"X\": X,\n",
    "        \"Y\": Y,\n",
    "        \"query\": query,\n",
    "        \"num_papers\": len(papers),\n",
    "    }\n",
    "    with open(os.path.join(output_dir, \"metadata.json\"), \"w\") as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "\n",
    "    # ---- save plot ----\n",
    "    fig = plot_literature_graph(lit_graph)\n",
    "    fig_path = os.path.join(output_dir, \"literature_graph.png\")\n",
    "    fig.savefig(fig_path, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "    # ---- console output ----\n",
    "    print(f\"\\nSaved results to: {output_dir}\")\n",
    "    print(\"\\nVERDICT:\", result[\"verdict\"])\n",
    "    print(\"\\nUNCERTAINTY:\", result[\"uncertainty\"])\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89117bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [\n",
    "    (\"alkali cations\", \"metal cations\"),\n",
    "    (\"alkali cations\", \"crystallographic lattice parameters\"),\n",
    "    (\"alkali cations\", \"lattice angle\"),\n",
    "    (\"alkali cations\", \"unit cell volume\"),\n",
    "    (\"alkali cations\", \"in-plane polarization\"),\n",
    "\n",
    "    (\"metal cations\", \"crystallographic lattice parameters\"),\n",
    "    (\"metal cations\", \"lattice angle\"),\n",
    "    (\"metal cations\", \"unit cell volume\"),\n",
    "    (\"metal cations\", \"in-plane polarization\"),\n",
    "\n",
    "    (\"crystallographic lattice parameters\", \"lattice angle\"),\n",
    "    (\"crystallographic lattice parameters\", \"unit cell volume\"),\n",
    "    (\"crystallographic lattice parameters\", \"in-plane polarization\"),\n",
    "\n",
    "    (\"lattice angle\", \"unit cell volume\"),\n",
    "    (\"lattice angle\", \"in-plane polarization\"),\n",
    "\n",
    "    (\"unit cell volume\", \"in-plane polarization\"),\n",
    "]\n",
    "\n",
    "\n",
    "for X, Y in pairs:\n",
    "    run_and_save_literature_reasoning(client, X, Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
